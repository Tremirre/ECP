{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd3jynF89haJ"
   },
   "source": [
    "# Evolutionary Computation - Assignment 8: Global convexity (fitness-distance/similarity correlations) tests\n",
    "Bartosz Stachowiak 148259<br>\n",
    "Andrzej Kajdasz 148273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANGmPjsk9haM"
   },
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "There are columns of integers representing nodes. Each row corresponds to a node and contains its x and y coordinates in a plane, as well as a cost associated with the node. There were 4 such data sets each consisting of 200 rows (each representing a single node).\n",
    "\n",
    "Problem to solve is to choose precisely 50% of the nodes (rounding up if there is an odd number of nodes) and create a Hamiltonian cycle (a closed path) using this subset of nodes. The goal is to minimize the combined total length of the path and the total cost of the selected nodes.\n",
    "\n",
    "To calculate the distances between nodes, the Euclidean distance formula was used and then round the results to the nearest integer. As suggested, the distances between the nodes were calculated after loading the data and placed in a matrix, so that during the subsequent evaluation of the problem, it was only necessary to read these values which reduced the cost of the operation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuHHtRw49haO"
   },
   "source": [
    "## 2. Results of the computational experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhyuKXc79haP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmY2J3gW9haQ"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "OLD_RESULTS_FOLDER = f'{DATA_FOLDER}old_results/'\n",
    "RESULT_FOLDER = f'{DATA_FOLDER}results/'\n",
    "INSTANCE_FOLDER = f'{DATA_FOLDER}tsp_instances/'\n",
    "\n",
    "SOLVERS = {\n",
    "    \"lsp-r\" : \"GreedyImprover\",\n",
    "}\n",
    "\n",
    "OLD_SOLVERS = {\n",
    "    \"lsnp-10-r\" : \"LNS Steepest LS (D10)\",\n",
    "    \"lsnp-20-r\" : \"LNS Steepest LS (D20)\",\n",
    "    \"lsnp-30-r\" : \"LNS Steepest LS (D30)\",\n",
    "    \"lsnp-50-r\" : \"LNS Steepest LS (D50)\",\n",
    "}\n",
    "SOLVERS_TO_PLOT = SOLVERS.copy()\n",
    "SOLVERS = {**OLD_SOLVERS, **SOLVERS}\n",
    "NUM_NODES = 200\n",
    "\n",
    "instance_files = [path for path in pathlib.Path(INSTANCE_FOLDER).iterdir() if path.is_file()]\n",
    "instance_names = [path.name[:4] for path in instance_files]\n",
    "p_sizes = [3, 5, 10, 20, 30, 50, 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR3Bfr4w9haQ"
   },
   "outputs": [],
   "source": [
    "instances_data = {\n",
    "    name: read_instance(f'{INSTANCE_FOLDER}{name}.csv')\n",
    "    for name in instance_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcmhV8nH9haQ"
   },
   "outputs": [],
   "source": [
    "instances_solvers_pairs = itertools.product(instances_data.keys(), SOLVERS.keys())\n",
    "\n",
    "all_results = {}\n",
    "all_costs = {}\n",
    "all_times = {}\n",
    "all_stats = {}\n",
    "\n",
    "for instance, solver in instances_solvers_pairs:\n",
    "    all_results[instance] = all_results.get(instance, {})\n",
    "    all_costs[instance] = all_costs.get(instance, {})\n",
    "    all_times[instance] = all_times.get(instance, {})\n",
    "    all_stats[instance] = all_stats.get(instance, {})\n",
    "    costs = []\n",
    "    times = []\n",
    "    paring_results = []\n",
    "    NUMBER_OF_FILES = 20 if solver in OLD_SOLVERS else 1000\n",
    "    for idx in range(NUMBER_OF_FILES):\n",
    "        folder = OLD_RESULTS_FOLDER if solver in OLD_SOLVERS else RESULT_FOLDER\n",
    "        if solver in OLD_SOLVERS:\n",
    "            solution, cost, time, no_iterations = read_solution_three_feature(f'{folder}{instance}-{solver}-{idx}.txt')\n",
    "        else:\n",
    "            solution, cost, time = read_solution(f'{folder}{instance}-{solver}-{idx}.txt')\n",
    "        paring_results.append(solution)\n",
    "        costs.append(cost)\n",
    "        times.append(time)\n",
    "        \n",
    "    all_results[instance][solver] = np.array(paring_results)\n",
    "    all_costs[instance][solver] = np.array(costs)\n",
    "    all_stats[instance][solver] = {\n",
    "        'mean': np.mean(costs),\n",
    "        'std': np.std(costs),\n",
    "        'min': np.min(costs),\n",
    "        'max': np.max(costs),\n",
    "    }\n",
    "    all_times[instance][solver] = {\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zy6WhlM9haR"
   },
   "outputs": [],
   "source": [
    "costs_df = pd.DataFrame(all_stats).T\n",
    "time_df = pd.DataFrame(all_times).T\n",
    "max_df = pd.DataFrame(all_stats).T\n",
    "min_df = pd.DataFrame(all_stats).T\n",
    "mean_time_df = pd.DataFrame(all_times).T\n",
    "\n",
    "for column in SOLVERS.keys():\n",
    "    costs_df[column] = costs_df[column].apply(lambda x: f'{x[\"mean\"]:.0f} ({x[\"min\"]:.0f} - {x[\"max\"]:.0f})')\n",
    "    time_df[column] = time_df[column].apply(lambda x: f'{x[\"mean\"]/1000000:.2f} ({x[\"min\"]/1000000:.2f} - {x[\"max\"]/1000000:.2f})')\n",
    "    max_df[column] = max_df[column].apply(lambda x: x['max'])\n",
    "    min_df[column] = min_df[column].apply(lambda x: x['min'])\n",
    "    mean_time_df[column] = mean_time_df[column].apply(lambda x: x['mean']/1000000)\n",
    "    \n",
    "for df in [costs_df, time_df, max_df, min_df, mean_time_df]:\n",
    "    df.rename(columns=SOLVERS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean (min-max) of the costs:\")\n",
    "\n",
    "best_means = {\n",
    "    instance: min(all_stats[instance][solver]['mean'] for solver in SOLVERS.keys())\n",
    "    for instance in instance_names\n",
    "}\n",
    "\n",
    "def apply_style(v: str, best_val: float):\n",
    "    num = v.split()[0]\n",
    "    try:\n",
    "        num = float(num)\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "    if round(num) == round(best_val):\n",
    "        return \"font-weight: bold; color: red\"\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "\n",
    "costs_df.T.style.apply(lambda x: [\n",
    "    apply_style(v, best_means[x.index[i]])\n",
    "    for i, v in enumerate(x)\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FgpBbE19haS"
   },
   "source": [
    "# 3. Best solutions for all datasets and algorithms\n",
    "\n",
    "To more easily compare the results, we present the best solutions for each dataset side by side.\n",
    "\n",
    "The weight of each node is denoted both by its size and color. The bigger and brighter the node, the higher its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    best_cost =  np.inf\n",
    "    for solver_idx, solver in enumerate(SOLVERS.keys()):\n",
    "         if best_cost > np.min(all_costs[instance][solver]):\n",
    "                best_cost = np.min(all_costs[instance][solver])\n",
    "                best_result = all_results[instance][solver][np.argmin(all_costs[instance][solver])], \n",
    "                best_solver = solver\n",
    "    best_instance_idx = np.argmin(all_costs[instance][best_solver])\n",
    "    plot_solution_for_instance(instances_data[instance], all_results[instance][best_solver][best_instance_idx], axs[idx])\n",
    "    axs[idx].set_title(f'{instance}: {all_costs[instance][best_solver][best_instance_idx]:.0f}')\n",
    "    print(instance)\n",
    "    print(f'\\tSolver: {SOLVERS[best_solver]}, Total cost: {best_cost}')\n",
    "    nodes = list(best_result[0])\n",
    "    if 0 in best_result[0]:\n",
    "        zero_index = np.where(best_result[0] == 0)[0][0]\n",
    "        nodes = list(best_result[0][zero_index:])+list(best_result[0][:zero_index])\n",
    "    print(f'\\t Nodes: {nodes}\\n')\n",
    "    best_results[instance] = {\"cost\": best_cost, \"nodes\": nodes}\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Similarity and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_similarity(x, y):\n",
    "    return len(x.intersection(y))\n",
    "\n",
    "def make_edges_from_nodes(nodes):\n",
    "    edge = [nodes[0], nodes[-1]]\n",
    "    edge.sort()\n",
    "    edges = [tuple(edge)]\n",
    "    for i in range(len(nodes)-1):\n",
    "        edge = [nodes[i], nodes[i+1]]\n",
    "        edge.sort()\n",
    "        edges.append(tuple(edge))\n",
    "    return set(edges)\n",
    "\n",
    "def prepare_data(nodes):\n",
    "    return (set(nodes), make_edges_from_nodes(nodes))\n",
    "\n",
    "def pearson_correlation(x,y):\n",
    "    return np.round(np.corrcoef(x, y)[0][1], 3)\n",
    "\n",
    "def plot_similarity(similarity, title, similarity_y_label):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 11), sharey = True)\n",
    "    axs[0][0].scatter(\n",
    "        similarity[\"TSPA\"][\"costs\"],\n",
    "        similarity[\"TSPA\"][\"similarity\"],\n",
    "    )\n",
    "    axs[0][1].scatter(\n",
    "        similarity[\"TSPB\"][\"costs\"],\n",
    "        similarity[\"TSPB\"][\"similarity\"],\n",
    "    )\n",
    "\n",
    "    axs[1][0].scatter(\n",
    "        similarity[\"TSPC\"][\"costs\"],\n",
    "        similarity[\"TSPC\"][\"similarity\"],\n",
    "    )\n",
    "    axs[1][1].scatter(\n",
    "        similarity[\"TSPD\"][\"costs\"],\n",
    "        similarity[\"TSPD\"][\"similarity\"],\n",
    "    )\n",
    "    \n",
    "    correlation = pearson_correlation(similarity[\"TSPA\"][\"costs\"], similarity[\"TSPA\"][\"similarity\"])\n",
    "    axs[0][0].set_title(f\"TSPA, correlation {correlation}\")\n",
    "    axs[0][0].set_xlabel(\"Cost\")\n",
    "    axs[0][0].set_ylabel(f\"Number of common {similarity_y_label}\")\n",
    "    \n",
    "    correlation = pearson_correlation(similarity[\"TSPB\"][\"costs\"], similarity[\"TSPB\"][\"similarity\"])\n",
    "    axs[0][1].set_title(f\"TSPB, correlation {correlation}\")\n",
    "    axs[0][1].set_xlabel(\"Cost\")\n",
    "    axs[0][1].set_ylabel(f\"Number of common {similarity_y_label}\")\n",
    "\n",
    "    correlation = pearson_correlation(similarity[\"TSPC\"][\"costs\"], similarity[\"TSPC\"][\"similarity\"])\n",
    "    axs[1][0].set_title(f\"TSPC, correlation {correlation}\")\n",
    "    axs[1][0].set_xlabel(\"Cost\")\n",
    "    axs[1][0].set_ylabel(f\"Number of common {similarity_y_label}\")\n",
    "\n",
    "    correlation = pearson_correlation(similarity[\"TSPD\"][\"costs\"], similarity[\"TSPD\"][\"similarity\"])\n",
    "    axs[1][1].set_title(f\"TSPD, correlation {correlation}\")\n",
    "    axs[1][1].set_xlabel(\"Cost\")\n",
    "    axs[1][1].set_ylabel(f\"Number of common {similarity_y_label}\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = {}\n",
    "all_edges = {}\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    nodes, edges = prepare_data(best_results[instance][\"nodes\"])\n",
    "    best_results[instance][\"set_nodes\"] = nodes\n",
    "    best_results[instance][\"edges\"] = edges\n",
    "    \n",
    "    all_nodes[instance] = []\n",
    "    all_edges[instance] = []\n",
    "    for i in all_results[instance][\"lsp-r\"]:\n",
    "        nodes, edges = prepare_data(i)\n",
    "        all_nodes[instance].append(nodes)\n",
    "        all_edges[instance].append(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Node similarity between best result and all other local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = {}\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    items = []\n",
    "    similarity[instance] = {}\n",
    "    for i in range (len(all_results[instance][\"lsp-r\"])):\n",
    "        items.append([all_costs[instance][\"lsp-r\"][i], common_similarity(all_nodes[instance][i], best_results[instance][\"set_nodes\"])])\n",
    "    similarity[instance][\"costs\"] = [n[0] for n in items]\n",
    "    similarity[instance][\"similarity\"] = [n[1] for n in items]\n",
    "plot_similarity(similarity, \"Node similarity between best result and all other local optima\", \"nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Edge similarity between best result and all other local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = {}\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    items = []\n",
    "    similarity[instance] = {}\n",
    "    for i in range (len(all_results[instance][\"lsp-r\"])):\n",
    "        items.append([all_costs[instance][\"lsp-r\"][i], common_similarity(all_edges[instance][i], best_results[instance][\"edges\"])])\n",
    "    similarity[instance][\"costs\"] = [n[0] for n in items]\n",
    "    similarity[instance][\"similarity\"] = [n[1] for n in items]\n",
    "plot_similarity(similarity, \"Edge similarity between best result and all other local optima\", \"edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Node similarity between all other local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = {}\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    items = []\n",
    "    similarity[instance] = {}\n",
    "    for i in range (len(all_results[instance][\"lsp-r\"])):\n",
    "        sum_similarity = 0\n",
    "        for j in range (len(all_results[instance][\"lsp-r\"])):\n",
    "            if i != j:\n",
    "                sum_similarity += common_similarity(all_nodes[instance][i], all_nodes[instance][j])\n",
    "        items.append([all_costs[instance][\"lsp-r\"][i], sum_similarity/999])\n",
    "    similarity[instance][\"costs\"] = [n[0] for n in items]\n",
    "    similarity[instance][\"similarity\"] = [n[1] for n in items]\n",
    "plot_similarity(similarity, \"Node similarity between all local optima\", \"nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Edge similarity between all other local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = {}\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    items = []\n",
    "    similarity[instance] = {}\n",
    "    for i in range (len(all_results[instance][\"lsp-r\"])):\n",
    "        sum_similarity = 0\n",
    "        for j in range (len(all_results[instance][\"lsp-r\"])):\n",
    "            if i != j:\n",
    "                sum_similarity += common_similarity(all_edges[instance][i], all_edges[instance][j])\n",
    "        items.append([all_costs[instance][\"lsp-r\"][i], sum_similarity/999])\n",
    "    similarity[instance][\"costs\"] = [n[0] for n in items]\n",
    "    similarity[instance][\"similarity\"] = [n[1] for n in items]\n",
    "plot_similarity(similarity, \"Edge similarity between all local optima\", \"edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YttFbeZ9haS"
   },
   "source": [
    "## 5. Source Code\n",
    "\n",
    "[GitHub](https://github.com/Tremirre/ECP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ZLJh009haS"
   },
   "source": [
    "## 6. Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results and visualizations, one can come to several conclusions about the algorithms used in the task:\n",
    "- For each of the cases presented, it can be seen that there is a negative correlation between the number of common nodes/edges and cost. \n",
    "- The correlation between the best solution found (LNS Steepest LS) and all local optima created by GreedyImprover:\n",
    "    - for node similarity is medium, ranging between -0.679 and -0.711 depending on the instance. The number of common nodes ranges from 85-95 for TSPA and TSPB, and 75-90 for TSPC and TSPD.\n",
    "    - for similarity calculated on the basis of shared edges, the correlation is lower (-0.528 to -0.611). There is a noticeable decrease in similarity as the number of common edges rarely exceeds 70. \n",
    "- The correlation between all local optima\n",
    "    - in case of node similarity is the most diverse (-0.477 to -0.652). This is despite the fact that the average numbers of all common nodes are quite centralized and in each instance the MAX-MIN difference is less than 10.\n",
    "    - for edge similarity the strongest correlation can be observed (-0.651 to -0.747). \n",
    "- It can be concluded that because most of the solutions found by GreedyImprover consist of many common nodes and the costs vary, the edges are more important and more correlated with the cost."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
