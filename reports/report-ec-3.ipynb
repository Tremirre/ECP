{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd3jynF89haJ"
   },
   "source": [
    "# Evolutionary Computation - Assignment 3: Local Search\n",
    "\n",
    "Bartosz Stachowiak 148259<br>\n",
    "Andrzej Kajdasz 148273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANGmPjsk9haM"
   },
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "There are columns of integers representing nodes. Each row corresponds to a node and contains its x and y coordinates in a plane, as well as a cost associated with the node. There were 4 such data sets each consisting of 200 rows (each representing a single node).\n",
    "\n",
    "Problem to solve is to choose precisely 50% of the nodes (rounding up if there is an odd number of nodes) and create a Hamiltonian cycle (a closed path) using this subset of nodes. The goal is to minimize the combined total length of the path and the total cost of the selected nodes.\n",
    "\n",
    "To calculate the distances between nodes, the Euclidean distance formula was used and then round the results to the nearest integer. As suggested, the distances between the nodes were calculated after loading the data and placed in a matrix, so that during the subsequent evaluation of the problem, it was only necessary to read these values which reduced the cost of the operation of the algorithm.\n",
    "\n",
    "To solve the problem the local search alogrithm was used with different configurations:\n",
    "- Type of local search:\n",
    "  - greedy - with random order ensured by shuffling the vector of all possible moves (inter and intra)\n",
    "  - steepest\n",
    "- Type of intra neighborhood:\n",
    "  - edge\n",
    "  - node\n",
    "  - edge + node (both)\n",
    "- Type of starting solutions:\n",
    "  - random\n",
    "  - best greedy heuristic (Weighted Greedy Regret Heuristic (0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXwJo02J9haN"
   },
   "source": [
    "## 2. Pseudocode of all implemented algorithms\n",
    "\n",
    "### Local Greedy Search\n",
    "\n",
    "```\n",
    "function local_greedy_search(solution, nodes, distance_matrix, neighborhood_type):\n",
    "    operations = generate_possible_operations(solution, len(nodes), neighborhood_type)\n",
    "    \n",
    "    while true:\n",
    "        // randomization of the operations is ensured by shuffling\n",
    "        shuffle(operations)\n",
    "        selected_operation = None\n",
    "        for operation in operations:\n",
    "            if op.evaluate_delta(solution, nodes, distance_matrix) < 0:\n",
    "                selected_operation = operation\n",
    "                break\n",
    "        if selected_operation is None:\n",
    "            break\n",
    "        \n",
    "        // When exchanging nodes between solution and outside of the solution,\n",
    "        // we need to update the operations list.\n",
    "        update_operations_list(operations, selected_operation)\n",
    "        \n",
    "        solution = selected_operation.apply(solution)\n",
    "\n",
    "    return solution\n",
    "```\n",
    "\n",
    "### Local Steepest Search\n",
    "\n",
    "```\n",
    "function local_steepest_search(solution, nodes, distance_matrix, neighborhood_type):\n",
    "    operations = generate_possible_operations(solution, len(nodes), neighborhood_type)\n",
    "    \n",
    "    while true:\n",
    "        best_operation = None\n",
    "        best_delta = 0\n",
    "        for operation in operations:\n",
    "            delta = op.evaluate_delta(solution, nodes, distance_matrix)\n",
    "            if delta < best_delta:\n",
    "                best_operation = operation\n",
    "                best_delta = delta\n",
    "\n",
    "        if best_operation is None:\n",
    "            break\n",
    "        \n",
    "        // When exchanging nodes between solution and outside of the solution,\n",
    "        // we need to update the operations list.\n",
    "        update_operations_list(operations, selected_operation)\n",
    "        \n",
    "        solution = selected_operation.apply(solution)\n",
    "\n",
    "    return solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuHHtRw49haO"
   },
   "source": [
    "## 3. Results of the computational experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA3NCQgm9haO"
   },
   "source": [
    "### 3.1. Code for visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhyuKXc79haP"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmY2J3gW9haQ"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "OLD_RESULTS_FOLDER = f'{DATA_FOLDER}old_results/'\n",
    "RESULT_FOLDER = f'{DATA_FOLDER}results/'\n",
    "INSTANCE_FOLDER = f'{DATA_FOLDER}tsp_instances/'\n",
    "\n",
    "SOLVERS = {\n",
    "    'r': \"Random\",\n",
    "    'd': \"Weighted Greedy Regret Heuristic (0.25)\",\n",
    "    'lssnode-r' : \"Steepest LS, node (random)\",\n",
    "    'lssedge-r' : \"Steepest LS, edge (random)\",\n",
    "    'lssboth-r' : \"Steepest LS, both (random)\",\n",
    "    'lsgnode-r' : \"Greedy LS, node (random)\",\n",
    "    'lsgedge-r' : \"Greedy LS, edge (random)\",\n",
    "    'lsgboth-r' : \"Greedy LS, both (random)\",\n",
    "    'lssnode-d' : \"Steepest LS, node (WGRH)\",\n",
    "    'lssedge-d' : \"Steepest LS, edge (WGRH)\",\n",
    "    'lssboth-d' : \"Steepest LS, both (WGRH)\",\n",
    "    'lsgnode-d' : \"Greedy LS, node (WGRH)\",\n",
    "    'lsgedge-d' : \"Greedy LS, edge (WGRH)\",\n",
    "    'lsgboth-d' : \"Greedy LS, both (WGRH)\",\n",
    "}\n",
    "\n",
    "OLD_SOLVERS = {\n",
    "    'n': \"Nearest Neighbor\",\n",
    "    'g': \"Greedy Cycle\",\n",
    "    'd-1': \"Greedy 2-regret heuristics\",\n",
    "    'd-1.75': \"Weighted greedy heuristics (0.75)\",\n",
    "}\n",
    "SOLVERS_TO_PLOT = SOLVERS.copy()\n",
    "SOLVERS_TO_PLOT.pop(\"r\")\n",
    "SOLVERS.update(OLD_SOLVERS)\n",
    "NUM_NODES = 200\n",
    "\n",
    "instance_files = [path for path in pathlib.Path(INSTANCE_FOLDER).iterdir() if path.is_file()]\n",
    "instance_names = [path.name[:4] for path in instance_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR3Bfr4w9haQ"
   },
   "outputs": [],
   "source": [
    "instances_data = {\n",
    "    name: read_instance(f'{INSTANCE_FOLDER}{name}.csv')\n",
    "    for name in instance_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcmhV8nH9haQ"
   },
   "outputs": [],
   "source": [
    "instances_solvers_pairs = itertools.product(instances_data.keys(), SOLVERS.keys())\n",
    "\n",
    "all_results = {}\n",
    "all_costs = {}\n",
    "all_times = {}\n",
    "all_stats = {}\n",
    "\n",
    "for instance, solver in instances_solvers_pairs:\n",
    "    all_results[instance] = all_results.get(instance, {})\n",
    "    all_costs[instance] = all_costs.get(instance, {})\n",
    "    all_times[instance] = all_times.get(instance, {})\n",
    "    all_stats[instance] = all_stats.get(instance, {})\n",
    "    costs = []\n",
    "    times = []\n",
    "    paring_results = []\n",
    "    for idx in range(NUM_NODES):\n",
    "        if(solver in OLD_SOLVERS):\n",
    "            time = 0\n",
    "            solution, cost = read_solution_timeless(f'{OLD_RESULTS_FOLDER}{instance}-{solver}-{idx}.txt')\n",
    "        else:\n",
    "            solution, cost, time = read_solution(f'{RESULT_FOLDER}{instance}-{solver}-{idx}.txt')\n",
    "        paring_results.append(solution)\n",
    "        costs.append(cost)\n",
    "        times.append(time)\n",
    "    all_results[instance][solver] = np.array(paring_results)\n",
    "    all_costs[instance][solver] = np.array(costs)\n",
    "    all_stats[instance][solver] = {\n",
    "        'mean': np.mean(costs),\n",
    "        'std': np.std(costs),\n",
    "        'min': np.min(costs),\n",
    "        'max': np.max(costs),\n",
    "    }\n",
    "    all_times[instance][solver] = {\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zy6WhlM9haR"
   },
   "outputs": [],
   "source": [
    "costs_df = pd.DataFrame(all_stats).T\n",
    "time_df = pd.DataFrame(all_times).T\n",
    "max_df = pd.DataFrame(all_stats).T\n",
    "min_df = pd.DataFrame(all_stats).T\n",
    "mean_time_df = pd.DataFrame(all_times).T\n",
    "\n",
    "for column in SOLVERS.keys():\n",
    "    costs_df[column] = costs_df[column].apply(lambda x: f'{x[\"mean\"]:.0f} ({x[\"min\"]:.0f} - {x[\"max\"]:.0f})')\n",
    "    time_df[column] = time_df[column].apply(lambda x: f'{x[\"mean\"]/1000:.2f} ({x[\"min\"]/1000:.2f} - {x[\"max\"]/1000:.2f})')\n",
    "    max_df[column] = max_df[column].apply(lambda x: x['max'])\n",
    "    min_df[column] = min_df[column].apply(lambda x: x['min'])\n",
    "    mean_time_df[column] = mean_time_df[column].apply(lambda x: x['mean']/1000)\n",
    "\n",
    "for df in [costs_df, time_df, max_df, min_df, mean_time_df]:\n",
    "    df.rename(columns=SOLVERS, inplace=True)\n",
    "time_df = time_df.drop(columns = OLD_SOLVERS.values())\n",
    "mean_time_df = mean_time_df.drop(columns = OLD_SOLVERS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo5of3599haR"
   },
   "source": [
    "### 3.2. Visualizations and statistics of cost for all dataset-algorithm pairs\n",
    "\n",
    "In tabular form we present the Mean, Minimum and Maximum of the results of the algorithms for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhTk45H69haR"
   },
   "outputs": [],
   "source": [
    "print(\"Mean (min-max) of the costs:\")\n",
    "costs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fasltPV99haR"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8), sharey=True)\n",
    "\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    if idx%2 == 0:\n",
    "        axs[(idx//2)%2][idx%2].set_ylabel('Cost')\n",
    "    axs[(idx//2)%2][idx%2].set_title(instance)\n",
    "\n",
    "    axs[(idx//2)%2][idx%2].violinplot(\n",
    "        [all_costs[instance][solver] for solver in SOLVERS_TO_PLOT.keys()],\n",
    "        showmeans=True,\n",
    "    )\n",
    "\n",
    "    axs[(idx//2)%2][idx%2].set_xticks(range(1, len(SOLVERS_TO_PLOT.keys()) + 1))\n",
    "    if idx > 1:\n",
    "        axs[(idx//2)%2][idx%2].set_xticklabels(SOLVERS_TO_PLOT.values(), rotation=45, ha='right')\n",
    "    else :\n",
    "        axs[(idx//2)%2][idx%2].set_xticklabels([])\n",
    "\n",
    "plt.suptitle('Distribution of the costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualizations and statistics of running times for all dataset-algorithm pairs\n",
    "\n",
    "Note: The running times for non Local Search algorithms have been averaged over 100 runs.\n",
    "\n",
    "The times for the Local Search algorithms is not averaged and does not take into account the time for the creation of the initial (input) solution.\n",
    "\n",
    "Min/Mean Time for Random algorithm is inaccurate due to its very short running time (at times yielding 0 μs in our measurements - indicating that the time is too short to be measured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean (min-max) of the time [ms]:\")\n",
    "time_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(len(SOLVERS_TO_PLOT))\n",
    "bar_width = 0.8 / len(instances_data.keys())\n",
    "\n",
    "mean_time_plot_df = mean_time_df.drop(columns = \"Random\").T.sort_values(by=\"TSPA\", ascending=False).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8), sharey=True)\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    ax.bar(\n",
    "        x_range + idx * bar_width,\n",
    "        height=mean_time_plot_df.loc[instance].values,\n",
    "        width=bar_width,\n",
    "        label=instance,\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x_range + bar_width * (len(instances_data.keys()) - 1) / 2)\n",
    "ax.set_xticklabels(mean_time_plot_df.columns, rotation=45, ha='right')\n",
    "plt.title('Time per instance per solver')\n",
    "plt.ylabel('Running Time [ms]')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FgpBbE19haS"
   },
   "source": [
    "## 4. Best solutions for all datasets and algorithms\n",
    "\n",
    "To more easily compare the results, we present the best solutions for each dataset side by side.\n",
    "\n",
    "The weight of each node is denoted both by its size and color. The bigger and brighter the node, the higher its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Gqoff_69haS"
   },
   "outputs": [],
   "source": [
    "for solver_idx, solver in enumerate(SOLVERS_TO_PLOT.keys()):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    for idx, instance in enumerate(instances_data.keys()):\n",
    "        best_instance_idx = np.argmin(all_costs[instance][solver])\n",
    "        plot_solution_for_instance(instances_data[instance], all_results[instance][solver][best_instance_idx], axs[idx])\n",
    "        axs[idx].set_title(f'{instance}: {all_costs[instance][solver][best_instance_idx]:.0f}')\n",
    "    fig.suptitle(f'{SOLVERS_TO_PLOT[solver]}', fontsize=16, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YttFbeZ9haS"
   },
   "source": [
    "## 5. Source Code\n",
    "\n",
    "[GitHub](https://github.com/Tremirre/ECP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ZLJh009haS"
   },
   "source": [
    "## 6. Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results and visualizations, one can come to several conclusions about the algorithms used in the task:\n",
    "- Most local search algorithms obtained comparable results with two exceptions - both steepest and greedy with node neighborhood based on a random initial solution. Looking at the visualization of their solutions, one can see that their relatively poor performance may be due to their inability to get rid of very long edges. Starting with a random solution, they replace expensive nodes with cheaper ones, but in this way it finds a local minimum very quickly with no further possible moves in this type of neighborhood. This leads to the conclusion that it is **much better to consider edge-swapped neighbors** in the solution and that even for a random solution, which may contain expensive nodes, then setting them in the right order i.e. avoiding long edges, gives a better result.\n",
    "\n",
    "- It is also a very interesting phenomenon that for TSPA and TSPB the better results are for those solutions that started on random. This may mean that the Greedy Heuristic solution algorithm was often originally already relatively close to some final local optimum. This is also confirmed by the execution time of this type of local search. In such a situation, **the random solution allows for more exploration of the solution space**.\n",
    "\n",
    "- Analyzing the execution time of the algorithms themselves, it can be noted that **the steepest version of local search tends to be faster than its greedy counterpart**. Taking into account the objective function, i.e. the final costs, the results are almost identical, which may mean that very often they end up in a similar local optimum only that the greedy version performs more less significant substitutions.\n",
    "\n",
    "- Comparing the time efficiency of both greedy and local search approaches, it can be noted that **it's most time and cost efficient to start of with a greedy solution and then apply local search to it**.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
