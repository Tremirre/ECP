{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd3jynF89haJ"
   },
   "source": [
    "# Evolutionary Computation - Assignment 2: Greedy Regret Heuristics\n",
    "\n",
    "Bartosz Stachowiak 148259<br>\n",
    "Andrzej Kajdasz 148273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANGmPjsk9haM"
   },
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "There were are columns of integers representing nodes. Each row corresponds to a node and contains its x and y coordinates in a plane, as well as a cost associated with the node. There were 4 such data sets each consisting of 200 rows (each representing a single node).\n",
    "\n",
    "Problem to solve is to choose precisely 50% of the nodes (rounding up if there is an odd number of nodes) and create a Hamiltonian cycle (a closed path) using this subset of nodes. The goal is to minimize the combined total length of the path and the total cost of the selected nodes.\n",
    "\n",
    "To calculate the distances between nodes, the Euclidean distance formula was used and then round the results to the nearest integer. As suggested, the distances between the nodes were calculated after loading the data and placed in a matrix, so that during the subsequent evaluation of the problem, it was only necessary to read these values which reduced the cost of the operation of the algorithm.\n",
    "\n",
    "Two approach were used to solve the problem:\n",
    "\n",
    "- Greedy 2-regret heuristics.\n",
    "- Greedy heuristics with a weighted sum criterion\n",
    "\n",
    "### Important note\n",
    "In case of greedy heuristics with a weighted sum criterion algorithm we are using a naming convention \"Weighted greedy heuristics (x)\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXwJo02J9haN"
   },
   "source": [
    "## 2. Pseudocode of all implemented algorithms\n",
    ">Note that in our implementation, distance matrix is not passed as an argument, but a class member, calculated before the algorithm is run.\\\n",
    ">For simplification, here we pass it as an argument.\n",
    "\n",
    "In our implementation, we noticed that by transforming the formula for the algorithm using weights, we can both optimize the number of calculations and - with the choice of appropriate weights - obtain a 2-regret algorithm:\n",
    "\n",
    "Let:\n",
    "- $m_1$ - minimum increase\n",
    "- $m_2$ - second minimum increase\n",
    "- $w$ - weight factor favoring the first minimum increase\n",
    "\n",
    "Then:\n",
    "- 2-regret is the maximization of $m_2 - m_1$\n",
    "- weighted sum is the maximization of $(2 - w) * m_2 - w * m_1$\n",
    "\n",
    "Hence we can use the same algorithm for both cases, by passing the appropriate weights.\n",
    "- For standard **2-regret**, we pass $w = 1$\n",
    "- For $w = 2$ we get maximization of $-2*m_1$ which is equivalent to minimization of $m_1$ - which boils down to the vanilla **Greedy Cycle Algorithm**\n",
    "\n",
    "Thus we provide pseudocode only for the weighted version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7N6HjSL9haN"
   },
   "source": [
    "\n",
    "```\n",
    "function greedy_heuristics_with_weighted_criterion(nodes, start_index, target_size, distance_matrix, weight_of_min_increase):\n",
    "\n",
    "    solution := [start_index]\n",
    "    visited := [false, false, ..., false]\n",
    "    visited[start_index] := true\n",
    "\n",
    "    // while solution is not complete, add nodes\n",
    "    while len(solution) < target_size:\n",
    "\n",
    "        best_weighted_criterion := -infinity\n",
    "        best_index := -1\n",
    "        insert_point := -1\n",
    "\n",
    "        if len(solution) == 1:\n",
    "            // find cheapest neighbor\n",
    "            min_increase = infinity\n",
    "            for i in 0..len(nodes) - 1:\n",
    "                if visited[i]:\n",
    "                    continue\n",
    "                if nodes[i].weight < min_increase:\n",
    "                    min_increase := nodes[i].cost\n",
    "                    best_index := i\n",
    "\n",
    "            solution.append(best_index)\n",
    "            visited[best_index] := true\n",
    "            continue\n",
    "\n",
    "        for i in 0..len(nodes) - 1:\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            \n",
    "            added_idx = -1;\n",
    "            min_increase = infinity\n",
    "            second_min_increase = infinity\n",
    "            \n",
    "            // find two related cheapest nodes\n",
    "            for j in 0..len(solution) - 1:\n",
    "\n",
    "                next_idx := (j + 1) % len(solution)\n",
    "                added_distance := distance_matrix[solution[j]][i] + distance_matrix[i][solution[next_idx]]\n",
    "                removed_distance := distance_matrix[solution[j]][solution[next_idx]]\n",
    "                increase := added_distance + nodes[i].weight - removed_distance\n",
    "\n",
    "                if increase <= min_increase:\n",
    "                    second_min_increase := min_increase\n",
    "                    min_increase := increase\n",
    "                    added_index := j\n",
    "                else if increase < second_min_increase:\n",
    "                    second_min_increase := increase\n",
    "            \n",
    "            //calculate regret and comapre to max regret\n",
    "            weighted_criterion := (2 - weight_of_min_increase) * second_min_increase - weight_of_min_increase * min_increase\n",
    "            if weighted_criterion > best_weighted_criterion:\n",
    "                best_weighted_criterion := weighted_criterion\n",
    "                insert-point := i\n",
    "                best_index := added_idx\n",
    "                \n",
    "\n",
    "        solution.insert(insert_point, best_index)\n",
    "        visited[min_index] := true\n",
    "\n",
    "    return solution\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuHHtRw49haO"
   },
   "source": [
    "## 3. Results of the computational experiments\n",
    "\n",
    "For consistency with the task description, we use the following naming convention for the weighted algorithm:\n",
    "- *Greedy 2-regret heuristics* - for $w = 1$\n",
    "- *Weighted greedy heuristics (x)* - for $w = x + 1$ (as it denotes combination of 2-regret and best change of the objective function; $x$ is a weight of best change of the objective function and $(1-x)$ is a weight of 2-regret value)\n",
    "\n",
    "This notation is also used in the conclusions section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA3NCQgm9haO"
   },
   "source": [
    "### 3.1. Code for visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UhyuKXc79haP"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmY2J3gW9haQ"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data/'\n",
    "RESULT_FOLDER = f'{DATA_FOLDER}results/'\n",
    "INSTANCE_FOLDER = f'{DATA_FOLDER}tsp_instances/'\n",
    "\n",
    "SOLVERS = {\n",
    "    'r': \"Random\",\n",
    "    'n': \"Nearest Neighbor\",\n",
    "    'g': \"Greedy Cycle\",\n",
    "    \"d-0.5\": \"Weighted greedy heuristics (-0.5)\",\n",
    "    'd-1': \"Greedy 2-regret heuristics\",\n",
    "    'd-1.25': \"Weighted greedy heuristics (0.25)\",\n",
    "    'd-1.5': \"Weighted greedy heuristics (0.5)\",\n",
    "    'd-1.75': \"Weighted greedy heuristics (0.75)\",\n",
    "    'd-2': \"Weighted greedy heuristics (1.0)\",\n",
    "}\n",
    "NUM_NODES = 200\n",
    "\n",
    "instance_files = [path for path in pathlib.Path(INSTANCE_FOLDER).iterdir() if path.is_file()]\n",
    "instance_names = [path.name[:4] for path in instance_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR3Bfr4w9haQ"
   },
   "outputs": [],
   "source": [
    "instances_data = {\n",
    "    name: read_instance(f'{INSTANCE_FOLDER}{name}.csv')\n",
    "    for name in instance_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcmhV8nH9haQ"
   },
   "outputs": [],
   "source": [
    "instances_solvers_pairs = itertools.product(instances_data.keys(), SOLVERS.keys())\n",
    "\n",
    "all_results = {}\n",
    "all_costs = {}\n",
    "all_stats = {}\n",
    "\n",
    "for instance, solver in instances_solvers_pairs:\n",
    "    all_results[instance] = all_results.get(instance, {})\n",
    "    all_costs[instance] = all_costs.get(instance, {})\n",
    "    all_stats[instance] = all_stats.get(instance, {})\n",
    "    costs = []\n",
    "    paring_results = []\n",
    "    for idx in range(NUM_NODES):\n",
    "        solution, cost, time = read_solution(f'{RESULT_FOLDER}{instance}-{solver}-{idx}.txt')\n",
    "        paring_results.append(solution)\n",
    "        costs.append(cost)\n",
    "    all_results[instance][solver] = np.array(paring_results)\n",
    "    all_costs[instance][solver] = np.array(costs)\n",
    "    all_stats[instance][solver] = {\n",
    "        'mean': np.mean(costs),\n",
    "        'std': np.std(costs),\n",
    "        'min': np.min(costs),\n",
    "        'max': np.max(costs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zy6WhlM9haR"
   },
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame(all_stats).T\n",
    "max_df = pd.DataFrame(all_stats).T\n",
    "min_df = pd.DataFrame(all_stats).T\n",
    "\n",
    "for column in SOLVERS.keys():\n",
    "    mean_df[column] = mean_df[column].apply(lambda x: f'{x[\"mean\"]:.0f} Â± {x[\"std\"]:.0f}')\n",
    "    max_df[column] = max_df[column].apply(lambda x: x['max'])\n",
    "    min_df[column] = min_df[column].apply(lambda x: x['min'])\n",
    "\n",
    "for df in [mean_df, max_df, min_df]:\n",
    "    df.rename(columns=SOLVERS, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo5of3599haR"
   },
   "source": [
    "### 3.2. Visualizations and statistics for all dataset-algorithm pairs\n",
    "\n",
    "In tabular form we present the Mean, Standard Deviation, Minimum and Maximum of the results of the algorithms for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhTk45H69haR"
   },
   "outputs": [],
   "source": [
    "print(\"Mean and std of the costs:\")\n",
    "mean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fasltPV99haR"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(instances_data.keys()), figsize=(15, 5), sharey=True)\n",
    "\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    if idx == 0:\n",
    "        axs[idx].set_ylabel('Cost')\n",
    "    axs[idx].set_title(instance)\n",
    "    axs[idx].violinplot(\n",
    "        [all_costs[instance][solver] for solver in SOLVERS.keys()],\n",
    "        showmeans=True,\n",
    "    )\n",
    "    axs[idx].set_xticks(range(1, len(SOLVERS.keys()) + 1))\n",
    "    axs[idx].set_xticklabels(SOLVERS.values(), rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Distribution of the costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsbGRDzs9haR"
   },
   "outputs": [],
   "source": [
    "print(\"Max of the costs:\")\n",
    "max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5m19uc29haS"
   },
   "outputs": [],
   "source": [
    "print(\"Min of the costs:\")\n",
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlbgeVSJ9haS"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(instances_data.keys()), figsize=(15, 5), sharey=True)\n",
    "bar_width = 0.35\n",
    "\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    if idx == 0:\n",
    "        axs[idx].set_ylabel('Cost')\n",
    "    axs[idx].set_title(instance)\n",
    "\n",
    "    axs[idx].bar(\n",
    "        np.arange(len(SOLVERS.keys())),\n",
    "        max_df.loc[instance].values,\n",
    "        width=bar_width,\n",
    "        label='Maximal cost',\n",
    "    )\n",
    "    axs[idx].bar(\n",
    "        np.arange(len(SOLVERS.keys())) + bar_width,\n",
    "        min_df.loc[instance].values,\n",
    "        width=bar_width,\n",
    "        label='Minimal cost',\n",
    "    )\n",
    "\n",
    "    axs[idx].set_xticks(np.arange(len(SOLVERS.keys())) + bar_width / 2)\n",
    "    axs[idx].set_xticklabels(SOLVERS.values(), rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Maximal cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FgpBbE19haS"
   },
   "source": [
    "## 4. Best solutions for all datasets and algorithms\n",
    "\n",
    "To more easily compare the results, we present the best solutions for each dataset side by side.\n",
    "\n",
    "The weight of each node is denoted both by its size and color. The bigger and brighter the node, the higher its weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Gqoff_69haS"
   },
   "outputs": [],
   "source": [
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    for solver_idx, solver in enumerate([\"d-1\",\"d-1.25\",\"d-1.5\",\"d-1.75\"]):\n",
    "        best_instance_idx = np.argmin(all_costs[instance][solver])\n",
    "        plot_solution_for_instance(instances_data[instance], all_results[instance][solver][best_instance_idx], axs[solver_idx])\n",
    "        axs[solver_idx].set_title(f'{SOLVERS[solver]}: {all_costs[instance][solver][best_instance_idx]:.0f}')\n",
    "    fig.suptitle(f'Instance {instance}', fontsize=16, y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YttFbeZ9haS"
   },
   "source": [
    "## 5. Source Code\n",
    "\n",
    "[GitHub](https://github.com/Tremirre/ECP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ZLJh009haS"
   },
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "Analyzing the results and visualizations, one can come to several conclusions about the algorithms used in the task:\n",
    "\n",
    "- Greedy heuristics with a weighted sum criterion is an algorithm that combines greedy cycle and greedy 2-regret heuristics. It should be noted that if the weight of the best change of the objective function is 0 then the algorithm will start acting like greedy 2-regret heuristics, and when the weight of 2-regret factor is 0 then the algorithm will act like greedy cycle what is confirmed by the results we obtained.\n",
    "  \n",
    "- During experiments with the greedy heuristics with a weighted sum criterion, the following cases were tested:\n",
    "    - Equal weights (results represent by *Weighted greedy heuristics (0.5)*):\n",
    "       - Works much better than greedy 2-regret heuristics\n",
    "       - In comparison to Random, Nearest Neighbor and Greedy Cycle based on obtained minimum cost algorithm works better for all dataset with only one exception (for TSPA greedy cycle is slightly better). In case of avarage cost works also better but also with one exception (for TSPD mean value of Nearest Neighbor is lower). \n",
    "    - 2-regret weight bigger than weight best change of the objective function \"Weighted greedy heuristics (0.25)\":\n",
    "    - Works slightly worse for TSPA, TSPB and TSPC than equal weights, but for TSPD it has the best result in comparison to all other algorithms. \n",
    "    - 2-regret weight smaller than weight best change of the objective function \"Weighted greedy heuristics (0.75)\":\n",
    "       - Better than other algorithms for TSPB\n",
    "       - Very similar results to Greedy Cycle what is obvious based on first conclusion\n",
    "<br><br>\n",
    "- Referring to the second conclusion, it can be summarized that greedy heuristics with a weighted sum criterion algorithm enables to obtain better results than algorithms such as Random, NN, Greedy Cycle and 2-regret. However, it is very important to choose the right weights for a given data set (problem).\n",
    "  \n",
    "- Greedy 2-regret heuristics outperforms only the random algorithm, moreover it has high volatility. Potentially, this could be due to the fact that the algorithm tries to avoid adding the cheapest nodes at the expense of those whose addition at a later stage may prove more expensive. However, in a problem of this nature where not every vertex needs to be added to the solution this is not the best approach. Nevertheless, it is noticeable that using the value of 2-regret in the greedy cycle that is the greedy heuristics algorithm with a weighted sum criterion described above improves the overall result."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
