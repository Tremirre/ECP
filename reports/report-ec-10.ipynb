{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd3jynF89haJ"
   },
   "source": [
    "# Evolutionary Computation - Assignment 10: Own Method\n",
    "Bartosz Stachowiak 148259<br>\n",
    "Andrzej Kajdasz 148273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANGmPjsk9haM"
   },
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "There are columns of integers representing nodes. Each row corresponds to a node and contains its x and y coordinates in a plane, as well as a cost associated with the node. There were 4 such data sets each consisting of 200 rows (each representing a single node).\n",
    "\n",
    "Problem to solve is to choose precisely 50% of the nodes (rounding up if there is an odd number of nodes) and create a Hamiltonian cycle (a closed path) using this subset of nodes. The goal is to minimize the combined total length of the path and the total cost of the selected nodes.\n",
    "\n",
    "To calculate the distances between nodes, the Euclidean distance formula was used and then round the results to the nearest integer. As suggested, the distances between the nodes were calculated after loading the data and placed in a matrix, so that during the subsequent evaluation of the problem, it was only necessary to read these values which reduced the cost of the operation of the algorithm.\n",
    "\n",
    "As the algorith for the problem, we improved upon the Genetic Local Search from the previous assignment and tested the improved version on elite size varying between 5 and 50 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXwJo02J9haN"
   },
   "source": [
    "## 2. Adjustments to the Algorithm\n",
    "\n",
    "### 2.1. Motivation behind the changes\n",
    "\n",
    "Looking back at our results from the previous assignments, we noticed that one glarring issue that might have been the cause of the poor performance of our algorithm was the low number of iterations made by the GLS - the best results were obtained by the GLS with lowest population size (5), which coincidentally, had the highest number of iterations.\n",
    "\n",
    "We hypothesized that improving the time-efficiency of the algorithm and increasing the number of iterations will lead to better results.\n",
    "\n",
    "The algorithm had two places where it could be improved in this regard:\n",
    "- initialization of the population\n",
    "- recombination operator\n",
    "\n",
    "Testing various optimisations for the initialization of the population, such as:\n",
    "- simplification of the generation of created instances using GreedyCycleSearch directly instead of using Local Search on randomly generated instances,\n",
    "- loosening the requirement on uniqueness of the generated instances,\n",
    "\n",
    "we arrived at the conclusion that the setup used originally in the previous assignments was the best.\n",
    "\n",
    "Looking at the recombination operator, we noticed that our original approach (finding longest common subsequences and filling the rest of the path with the remaining nodes) was quite complex and took a lot of time. We speculated that simplification of the operator could lead to better results.\n",
    "\n",
    "After testing various different variations, the most efficient consisted of simply iterating over the 100 indices of a solution, and at every index, choosing the node with lower weight from the two parents. If the selected node was already present in the solution, we skipped it and moved on to the next index. Finally the solution was filled out using GreedyCycleSearch.\n",
    "\n",
    "### 2.2 Pseudocode of the new recombination operator\n",
    "\n",
    "```\n",
    "function recombine(parent_1, parent_2, nodes, distances):\n",
    "    child = []\n",
    "    for i in range(100):\n",
    "        first_parent_node = parent_1[i]\n",
    "        second_parent_node = parent_2[i]\n",
    "        \n",
    "        selected_node = first_parent_node\n",
    "        if nodes[selected_node].weight > nodes[second_parent_node].weight:\n",
    "            selected_node = second_parent_node\n",
    "        \n",
    "        if selected_node not in child:\n",
    "            child.append(selected_node)\n",
    "    \n",
    "    solver = GreedyCycleSearch(nodes, distances)\n",
    "    child = solver.solve(child)\n",
    "    return child\n",
    "```\n",
    "\n",
    "### 2.3. Fixing the bug in evaluation scripts\n",
    "\n",
    "When generating computational results for the EC assignments, we used powershell scripts that called the compiled c++ program with various parameters. The scripts were written in a way that they would run the program for a given set of parameters and then save the results to a file.\n",
    "\n",
    "To speed up the process of generating the results, we start many instances of the program in parallel, each with different parameters. This was done using the `ForEach -Parallel` powershell command, with the number of instances set to **48**.\n",
    "\n",
    "This number was chosen at the beginning, as it was sure to utilize all logical cores of the CPU, and was not changed afterwards. It did not cause any issues as the initial assignments did not limit the running time of the algorithm.\n",
    "\n",
    "However, as ILS, LNS and finally GLS were time-bounded, such a high number of instances running in parallel caused the results to be incorrect. This was due to the fact that the machine running the script had only **16** logical cores. This implies that the maximum number of instances that could be run in parallel without causing issues was **16**.\n",
    "\n",
    "For time bounding we used the `std::chrono` library, which is based on the system clock. This means that the time measured by the program is the time that has passed on the machine running the program. This means that if the machine is running multiple instances of the program in parallel, the time measured by the program takes into account the time spent on running other instances of the program.\n",
    "\n",
    "We fixed this issue by changing the number of instances run in parallel to **4** (a bit lower to make sure all measurements are not influenced). This allowed us to further improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuHHtRw49haO"
   },
   "source": [
    "## 3. Results of the computational experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhyuKXc79haP"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmY2J3gW9haQ"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "OLD_RESULTS_FOLDER = f'{DATA_FOLDER}old_results/'\n",
    "RESULT_FOLDER = f'{DATA_FOLDER}results/'\n",
    "INSTANCE_FOLDER = f'{DATA_FOLDER}tsp_instances/'\n",
    "\n",
    "SOLVERS = {\n",
    "    \"lsap-5-r\": \"Adjusted Genetic LS (Pop 5)\",\n",
    "    \"lsap-10-r\": \"Adjusted Genetic LS (Pop 10)\",\n",
    "    \"lsap-20-r\": \"Adjusted Genetic LS (Pop 20)\",\n",
    "    \"lsap-30-r\": \"Adjusted Genetic LS (Pop 30)\",\n",
    "    \"lsap-50-r\": \"Adjusted Genetic LS (Pop 50)\",\n",
    "}\n",
    "\n",
    "OLD_SOLVERS = {\n",
    "    \"lsnp-20-r\" : \"LNS Steepest LS (D20)\",\n",
    "    \"lsnp-50-r\" : \"LNS Steepest LS (D50)\",\n",
    "    \"lsep-5-r\": \"Genetic LS, with steepest post-recombine LS (Pop 5)\",\n",
    "}\n",
    "\n",
    "OLD_SOLVERS_NO_ITER = {\n",
    "    \"lsm-r\" : \"Steepest Multi Start LS\",\n",
    "    \"lsi-10-r\" : \"Iterated LS (Perturbation size 10)\",\n",
    "    \"lsi-20-r\" : \"Iterated LS (Perturbation size 20)\",\n",
    "}\n",
    "SOLVERS_TO_PLOT = SOLVERS.copy()\n",
    "OLD_SOLVERS = {**OLD_SOLVERS_NO_ITER, **OLD_SOLVERS}\n",
    "SOLVERS = {**OLD_SOLVERS, **SOLVERS}\n",
    "NUM_NODES = 200\n",
    "\n",
    "instance_files = [path for path in pathlib.Path(INSTANCE_FOLDER).iterdir() if path.is_file()]\n",
    "instance_names = [path.name[:4] for path in instance_files]\n",
    "p_sizes = [5, 10, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR3Bfr4w9haQ"
   },
   "outputs": [],
   "source": [
    "instances_data = {\n",
    "    name: read_instance(f'{INSTANCE_FOLDER}{name}.csv')\n",
    "    for name in instance_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcmhV8nH9haQ"
   },
   "outputs": [],
   "source": [
    "instances_solvers_pairs = itertools.product(instances_data.keys(), SOLVERS.keys())\n",
    "\n",
    "all_results = {}\n",
    "all_costs = {}\n",
    "all_times = {}\n",
    "all_stats = {}\n",
    "all_no_iterations = {}\n",
    "\n",
    "for instance, solver in instances_solvers_pairs:\n",
    "    all_results[instance] = all_results.get(instance, {})\n",
    "    all_costs[instance] = all_costs.get(instance, {})\n",
    "    all_times[instance] = all_times.get(instance, {})\n",
    "    all_stats[instance] = all_stats.get(instance, {})\n",
    "    all_no_iterations[instance] = all_no_iterations.get(instance, {})\n",
    "    costs = []\n",
    "    times = []\n",
    "    paring_results = []\n",
    "    iterations = []\n",
    "    for idx in range(20):\n",
    "        folder = OLD_RESULTS_FOLDER if solver in OLD_SOLVERS else RESULT_FOLDER\n",
    "        if solver in OLD_SOLVERS_NO_ITER:\n",
    "            solution, cost, time = read_solution(f'{folder}{instance}-{solver}-{idx}.txt')\n",
    "        else:\n",
    "            solution, cost, time, no_iterations = read_solution_three_feature(f'{folder}{instance}-{solver}-{idx}.txt')\n",
    "            iterations.append(no_iterations)\n",
    "        paring_results.append(solution)\n",
    "        costs.append(cost)\n",
    "        times.append(time)\n",
    "        \n",
    "    all_results[instance][solver] = np.array(paring_results)\n",
    "    all_costs[instance][solver] = np.array(costs)\n",
    "    all_stats[instance][solver] = {\n",
    "        'mean': np.mean(costs),\n",
    "        'std': np.std(costs),\n",
    "        'min': np.min(costs),\n",
    "        'max': np.max(costs),\n",
    "    }\n",
    "    all_times[instance][solver] = {\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "    }\n",
    "    if solver not in OLD_SOLVERS_NO_ITER:\n",
    "        all_no_iterations[instance][solver] = {\n",
    "            'mean': np.mean(iterations),\n",
    "            'std': np.std(iterations),\n",
    "            'min': np.min(iterations),\n",
    "            'max': np.max(iterations),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zy6WhlM9haR"
   },
   "outputs": [],
   "source": [
    "costs_df = pd.DataFrame(all_stats).T\n",
    "max_df = pd.DataFrame(all_stats).T\n",
    "min_df = pd.DataFrame(all_stats).T\n",
    "iterations_df = pd.DataFrame(all_no_iterations).T\n",
    "\n",
    "for column in SOLVERS.keys():\n",
    "    costs_df[column] = costs_df[column].apply(lambda x: f'{x[\"mean\"]:.0f} ({x[\"min\"]:.0f} - {x[\"max\"]:.0f})')\n",
    "    max_df[column] = max_df[column].apply(lambda x: x['max'])\n",
    "    min_df[column] = min_df[column].apply(lambda x: x['min'])\n",
    "    if column not in OLD_SOLVERS_NO_ITER:\n",
    "        iterations_df[column] = iterations_df[column].apply(lambda x: f'{x[\"mean\"]:.0f} ({x[\"min\"]:.0f} - {x[\"max\"]:.0f})')\n",
    "\n",
    "for df in [costs_df, max_df, min_df, iterations_df]:\n",
    "    df.rename(columns=SOLVERS, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo5of3599haR"
   },
   "source": [
    "### 3.1. Visualizations and statistics of cost for all dataset-algorithm pairs\n",
    "\n",
    "In tabular form we present the Mean, Minimum and Maximum of the results of the algorithms for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhTk45H69haR"
   },
   "outputs": [],
   "source": [
    "print(\"Mean (min-max) of the costs:\")\n",
    "\n",
    "best_means = {\n",
    "    instance: min(all_stats[instance][solver]['mean'] for solver in SOLVERS.keys())\n",
    "    for instance in instance_names\n",
    "}\n",
    "\n",
    "def apply_style(v: str, best_val: float):\n",
    "    num = v.split()[0]\n",
    "    try:\n",
    "        num = float(num)\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "    if round(num) == round(best_val):\n",
    "        return \"font-weight: bold; color: red\"\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "\n",
    "costs_df.T.style.apply(lambda x: [\n",
    "    apply_style(v, best_means[x.index[i]])\n",
    "    for i, v in enumerate(x)\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mean number of iterations\n",
    "\n",
    "Time limits:\n",
    "- TSPA: 9.12 s\n",
    "- TSPB: 8.62 s\n",
    "- TSPC: 6.5 s\n",
    "- TSPD: 5.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean (min-max) of the iterations:\")\n",
    "iterations_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3. Visualizations of the impact of population size on the iterations number and mean cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "for instance in instances_data.keys():\n",
    "    \n",
    "    axs[0].plot(\n",
    "        p_sizes,\n",
    "        [all_stats[instance][f\"lsap-{n}-r\"]['mean'] for n in p_sizes],\n",
    "        label=instance,\n",
    "        marker='o', \n",
    "        linestyle='dashed'\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        p_sizes,\n",
    "        [all_no_iterations[instance][f\"lsap-{n}-r\"]['mean'] for n in p_sizes],\n",
    "        label=instance,\n",
    "        marker='o', \n",
    "        linestyle='dashed'\n",
    "    )\n",
    "\n",
    "plt.suptitle(f'Genetic LS stats per population size')\n",
    "\n",
    "axs[0].set_title(\"Mean cost\")\n",
    "axs[0].set_xlabel('Size of population')\n",
    "axs[0].set_ylabel('Mean cost')\n",
    "\n",
    "axs[1].set_title('Number of iterations')\n",
    "axs[1].set_xlabel('Size of population')\n",
    "axs[1].set_ylabel('Number of iterations')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FgpBbE19haS"
   },
   "source": [
    "## 4. Best solutions for all datasets and algorithms\n",
    "\n",
    "To more easily compare the results, we present the best solutions for each dataset side by side.\n",
    "\n",
    "The weight of each node is denoted both by its size and color. The bigger and brighter the node, the higher its weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 New algortithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Gqoff_69haS"
   },
   "outputs": [],
   "source": [
    "for solver_idx, solver in enumerate(SOLVERS_TO_PLOT.keys()):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    for idx, instance in enumerate(instances_data.keys()):\n",
    "        best_instance_idx = np.argmin(all_costs[instance][solver])\n",
    "        plot_solution_for_instance(instances_data[instance], all_results[instance][solver][best_instance_idx], axs[idx])\n",
    "        axs[idx].set_title(f'{instance}: {all_costs[instance][solver][best_instance_idx]:.0f}')\n",
    "    fig.suptitle(f'{SOLVERS[solver]}', fontsize=16, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Best solution for each instance from all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, instance in enumerate(instances_data.keys()):\n",
    "    best_cost =  np.inf\n",
    "    for solver_idx, solver in enumerate(SOLVERS.keys()):\n",
    "         if best_cost > np.min(all_costs[instance][solver]):\n",
    "                best_cost = np.min(all_costs[instance][solver])\n",
    "                best_result = all_results[instance][solver][np.argmin(all_costs[instance][solver])], \n",
    "                best_solver = solver\n",
    "    best_instance_idx = np.argmin(all_costs[instance][best_solver])\n",
    "    plot_solution_for_instance(instances_data[instance], all_results[instance][best_solver][best_instance_idx], axs[idx])\n",
    "    axs[idx].set_title(f'{instance}: {all_costs[instance][best_solver][best_instance_idx]:.0f}')\n",
    "    print(instance)\n",
    "    print(f'\\tSolver: {SOLVERS[best_solver]}, Total cost: {best_cost}')\n",
    "    nodes = list(best_result[0])\n",
    "    if 0 in best_result[0]:\n",
    "        zero_index = np.where(best_result[0] == 0)[0][0]\n",
    "        nodes = list(best_result[0][zero_index:])+list(best_result[0][:zero_index])\n",
    "    print(f'\\t Nodes: {nodes}\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YttFbeZ9haS"
   },
   "source": [
    "## 5. Source Code\n",
    "\n",
    "[GitHub](https://github.com/Tremirre/ECP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ZLJh009haS"
   },
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "Analyzing the results and visualizations, one can come to several conclusions about the algorithms used in the task:\n",
    "- The algorithm created achieved better average results than all those implemented so far.\n",
    "- The exploration of the solution space in GLS provided by the number of iterations is more valuable than the one obtained with sophisticated recombination operators.\n",
    "- For the TSPA and TSPB instances, the best results obtained are identical to those obtained by Large Neighbourhood Search. So there is a chance that this local minimum is also the global optimum or very close to it. For TSPC, the minimum obtained is the best among the other algorithms. For the TSPD instance, the minimum score is 100 worse than that obtained by LNS.\n",
    "- Contrary to the results in previous assignment, now the best results increase with the increase of the population size.\n",
    "- Even though the improved algorithm decisively outperforms the previous ones, it still does not find better overall solutions for TSPA and TSPD (it only finds the same best solutions). This might suggest that these solutions are close to the global optimum."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
